{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def report_acc(data,label):\n",
    "    train_index = np.random.choice(len(label),int(len(label)/2),replace=False)\n",
    "    test_index = np.setdiff1d(np.arange(len(label)),train_index)\n",
    "\n",
    "    train_data = data[train_index]\n",
    "    train_label = label[train_index]\n",
    "    test_data = data[test_index]\n",
    "    test_label = label[test_index]\n",
    "\n",
    "    train_data = np.reshape(train_data, (len(train_label), -1))\n",
    "    test_data = np.reshape(test_data, (len(test_label),-1))\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    clf = LogisticRegression(random_state=0, solver='liblinear',class_weight='balanced')\n",
    "    clf.fit(train_data, train_label)\n",
    "    #print (\" attack accuracy %.2f\" % (clf.score(test_data, test_label) * 100))\n",
    "    acc1 = balanced_accuracy_score(test_label,clf.predict(test_data))\n",
    "    #print (classification_report(test_label,clf.predict(test_feature)))\n",
    "    auc1 = roc_auc_score(test_label,clf.predict(test_data))\n",
    "    f1_1 = f1_score(test_label,clf.predict(test_data), average='weighted')\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=100,max_depth=30, random_state=0,class_weight=\"balanced\")\n",
    "    \n",
    "    #from sklearn.svm import SVC\n",
    "    #clf = SVC(gamma='auto')\n",
    "    \n",
    "    #from sklearn.neural_network import MLPClassifier\n",
    "    #lf = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(10,5), random_state=1,max_iter=100)\n",
    "    \n",
    "    \n",
    "    clf.fit(train_data, train_label)\n",
    "    acc2 = balanced_accuracy_score(test_label,clf.predict(test_data))\n",
    "    auc2 = roc_auc_score(test_label,clf.predict(test_data))\n",
    "    f1_2 = f1_score(test_label,clf.predict(test_data), average='weighted')\n",
    "    #print (classification_report(test_label,mode.predict(test_feature)))\n",
    "    #print (\"balanced accuracy\",max(acc1,acc2))\n",
    "    #print (\"roc-auc\",max(auc1,auc2))\n",
    "    #print (\"f1 score\",max(f1_1,f1_2))\n",
    "\n",
    "    return max(acc1,acc2)\n",
    "    #max(auc1,auc2)\n",
    "    #max(f1_1,f1_2)\n",
    "\n",
    "def report_auc(data,label):\n",
    "    #print (\"report auc\")\n",
    "    #print (data.shape)\n",
    "    #print (label.shape)\n",
    "    \n",
    "    if (len(data.shape)==1):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        y_true = label\n",
    "        y_pred = np.squeeze(data)\n",
    "        #print (\"AUC score\", roc_auc_score(-1*y_true, y_pred))\n",
    "        return roc_auc_score(y_true,y_pred)\n",
    "    else:\n",
    "        ### \n",
    "        pass\n",
    "    \n",
    "def normalization(data):\n",
    "    data = (data - np.average(data))/np.std(data)\n",
    "    return data\n",
    "\n",
    "def processing_batch(data):\n",
    "    #num_batches = data.shape[0]\n",
    "    #print (data.shape)\n",
    "    num_instances = data.shape[0]\n",
    "    num_layers = data.shape[1]\n",
    "    num_features = data.shape[2]\n",
    "    #data = np.reshape(data,(-1,num_layers,num_features))\n",
    "    \n",
    "    \n",
    "    ### features in data: 0. cos 1. |B*n|_L1 2.|b*n-a|_L1 3.|a|_L1 4.L2 of 1 5.L2 of 2 6.L2 of 3  \n",
    "    ###                   7. pos_sign(param1_ground) 8.neg_sign(param1_ground) 9. pos_sign(param2_ground) 10.neg_sign(param2_ground)\n",
    "    ###                   11. pos_sign(counting) 12.neg_sign(counting) 13. sum_counting\n",
    "    ### sparse feat above 14.     15.          16.         17.      18.       19.       20.     \n",
    "    ###                   21.                        22.                       23.                         24.\n",
    "    ###                   25.                    26.                   27.\n",
    "    ### \n",
    "    \n",
    "    ### we have 5 features here: 0. cos 1. |b*n|-|b*n-a| L1 2. |b*n|-|b*n-a| L2 3. |a| L1 4. |a| L2 5. pos_sign(a-ground) \n",
    "    ###                          6.neg_sign(a-ground) 7.pos_sign(b_ground) 8.neg_sign(b_ground) 9. pos_sign(counting) \n",
    "    ###                          10.neg_sign(counting) 11. sum_counting\n",
    "    ### 12 for normal\n",
    "    ### 12 for sparse\n",
    "    ### this function is subject to change because we may have other ways to process the information from multiple batches\n",
    "    \n",
    "    generated_features = np.zeros((num_instances,num_layers,24))\n",
    "    \n",
    "    for i in range(num_instances):\n",
    "        this_instance_data = data[i,:,:]\n",
    "        \n",
    "        for j in range(num_layers):\n",
    "            generated_features[i,j,0] = this_instance_data[j,0]\n",
    "            generated_features[i,j,3] = this_instance_data[j,3]\n",
    "            generated_features[i,j,4] = this_instance_data[j,6]\n",
    "            generated_features[i,j,1] = this_instance_data[j,1]-this_instance_data[j,2]\n",
    "            generated_features[i,j,2] = this_instance_data[j,4]-this_instance_data[j,5]\n",
    "            generated_features[i,j,5] = this_instance_data[j,7]\n",
    "            generated_features[i,j,6] = this_instance_data[j,8]\n",
    "            generated_features[i,j,7] = this_instance_data[j,9]\n",
    "            generated_features[i,j,8] = this_instance_data[j,10]\n",
    "            generated_features[i,j,9] = this_instance_data[j,11]\n",
    "            generated_features[i,j,10] = this_instance_data[j,12]\n",
    "            generated_features[i,j,11] = this_instance_data[j,13]\n",
    "\n",
    "            generated_features[i,j,12] = this_instance_data[j,14]\n",
    "            generated_features[i,j,15] = this_instance_data[j,17]\n",
    "            generated_features[i,j,16] = this_instance_data[j,20]\n",
    "            generated_features[i,j,13] = this_instance_data[j,15]-this_instance_data[j,16]\n",
    "            generated_features[i,j,14] = this_instance_data[j,18]-this_instance_data[j,19]\n",
    "            generated_features[i,j,17] = this_instance_data[j,21]\n",
    "            generated_features[i,j,18] = this_instance_data[j,22]\n",
    "            generated_features[i,j,19] = this_instance_data[j,23]\n",
    "            generated_features[i,j,20] = this_instance_data[j,24]\n",
    "            generated_features[i,j,21] = this_instance_data[j,25]\n",
    "            generated_features[i,j,22] = this_instance_data[j,26]\n",
    "            generated_features[i,j,23] = this_instance_data[j,27]\n",
    "            \n",
    "    ### we can normalize the cosine and the norm per layer \n",
    "    \n",
    "    #for j in range(num_layers):\n",
    "    #    cos_normalized = normalization(generated_features[:,j,0])\n",
    "        #norm_normalized = normalization(genearted_features[:,j,4])\n",
    "    #    generated_features[:,j,0] = cos_normalized\n",
    "        #generated_features[:,j,4] = norm_normalized\n",
    "            \n",
    "    return generated_features\n",
    "    \n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    \n",
    "def layer_analysis(epochs,data_size,num_layers,evaluation_size,model_name,dataset_name,name_prefix,special_layers=None):\n",
    "    num_users = 3\n",
    "    batch_size = 100\n",
    "    num_layers = 12\n",
    "    for index,epoch in enumerate(epochs):\n",
    "        naming_str = '0_0.0_0_0.0_0_'+str(epoch)+'_'+str(dataset_name)+'_'+str(data_size)+'_'+str(evaluation_size)+'_'+str(model_name)+'.npy'\n",
    "        \n",
    "        data_name = name_prefix+naming_str\n",
    "        data = np.load(data_name,allow_pickle=True)\n",
    "        label = np.concatenate((np.ones((evaluation_size)),np.zeros((evaluation_size))))\n",
    "        this_user_label = label\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            for user_idx in range(num_users):\n",
    "                this_user_instance_info = processing_batch(data[user_idx,:,:,:])\n",
    "                #print (this_user_instance_info.shape)\n",
    "                member_index = np.arange(len(label))[label == 1]\n",
    "                nonmember_index = np.arange(len(label))[label == 0]\n",
    "                print (f\"layer {layer}, epoch {epoch}, user_idx {user_idx}\")\n",
    "                print (f\"member pos sign {this_user_instance_info[0,layer,7]}, member neg sign {this_user_instance_info[0,layer,8]}\")\n",
    "                print (f\"non-member pos sign {this_user_instance_info[-1,layer,7]}, non-member neg sign {this_user_instance_info[-1,layer,8]}\")\n",
    "            \n",
    "            #fig = plt.figure(figsize=(5,5))\n",
    "            #for i in range(2):\n",
    "            #    plt.subplot(1,2,i+1)\n",
    "            #    bin_num = 20\n",
    "            #    counts,bins = np.histogram(np.sum(this_user_instance_info[:,:,7+i][member_index],axis=1),bin_num)\n",
    "            #    plt.bar(bins[1:],counts,width= (bins[1] - bins[0])/2)\n",
    "            #    counts,_ = np.histogram(np.sum(this_user_instance_info[:,:,7+i][nonmember_index],axis=1),bins)\n",
    "            #    plt.bar(bins[1:] + (bins[1] - bins[0])/2,counts,width= (bins[1] - bins[0])/2)\n",
    "            #    plt.legend(['members','non-members'])\n",
    "            #    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0, epoch 10\n",
      "member pos sign 11825.0, member neg sign -11407.0\n",
      "non-member pos sign 11825.0, non-member neg sign -11407.0\n",
      "layer 0, epoch 10\n",
      "member pos sign 11401.0, member neg sign -11831.0\n",
      "non-member pos sign 11401.0, non-member neg sign -11831.0\n",
      "layer 0, epoch 10\n",
      "member pos sign 11563.0, member neg sign -11669.0\n",
      "non-member pos sign 11563.0, non-member neg sign -11669.0\n",
      "layer 1, epoch 10\n",
      "member pos sign 10.0, member neg sign -54.0\n",
      "non-member pos sign 10.0, non-member neg sign -54.0\n",
      "layer 1, epoch 10\n",
      "member pos sign 37.0, member neg sign -27.0\n",
      "non-member pos sign 37.0, non-member neg sign -27.0\n",
      "layer 1, epoch 10\n",
      "member pos sign 45.0, member neg sign -19.0\n",
      "non-member pos sign 45.0, non-member neg sign -19.0\n",
      "layer 2, epoch 10\n",
      "member pos sign 151988.0, member neg sign -155211.0\n",
      "non-member pos sign 151988.0, non-member neg sign -155211.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-24d373f54882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m layer_analysis(epochs,5000,12,5000,'alexnet','cifar10',name_prefix='/Users/jclialex/PycharmProjects/whiteboxmi_expdata/expdata/expdata/all_info_cross_member_single_epoch_',\n\u001b[0;32m----> 3\u001b[0;31m                special_layers=[0,2,4,6,8,10])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-cd99cbde6617>\u001b[0m in \u001b[0;36mlayer_analysis\u001b[0;34m(epochs, data_size, num_layers, evaluation_size, model_name, dataset_name, name_prefix, special_layers)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0muser_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mthis_user_instance_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0;31m#print (this_user_instance_info.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mmember_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-cd99cbde6617>\u001b[0m in \u001b[0;36mprocessing_batch\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mgenerated_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_instance_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mgenerated_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_instance_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mthis_instance_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mgenerated_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_instance_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mthis_instance_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mgenerated_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_instance_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mgenerated_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_instance_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = (np.arange(30)+1)*10\n",
    "layer_analysis(epochs,5000,12,5000,'alexnet','cifar10',name_prefix='/Users/jclialex/PycharmProjects/whiteboxmi_expdata/expdata/expdata/all_info_cross_member_single_epoch_',\n",
    "               special_layers=[0,2,4,6,8,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = (np.arange(30)+1)*10\n",
    "layer_analysis(epochs,5000,12,5000,'alexnet','cifar100',name_prefix='/Users/jclialex/PycharmProjects/whiteboxmi_expdata/expdata/expdata/all_info_cross_member_single_epoch_',\n",
    "               special_layers=[0,2,4,6,8,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
