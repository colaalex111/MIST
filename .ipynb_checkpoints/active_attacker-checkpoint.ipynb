{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   \n",
    "def report_acc(data,label):    \n",
    "    #print (data.shape)\n",
    "    unique_labels = np.unique(label)\n",
    "    train_index = []\n",
    "    for this_label in unique_labels:\n",
    "        this_class_index = np.arange(len(label))[label == this_label]\n",
    "        this_class_train_index = np.random.choice(this_class_index,int(len(this_class_index)/2),replace=False)\n",
    "        train_index.append(this_class_train_index)\n",
    "    train_index = np.reshape(np.array(train_index),(-1))\n",
    "    test_index = np.setdiff1d(np.arange(len(label)),train_index)\n",
    "\n",
    "    train_data = data[train_index]\n",
    "    train_label = label[train_index]\n",
    "    test_data = data[test_index]\n",
    "    test_label = label[test_index]\n",
    "\n",
    "    train_data = np.reshape(train_data, (len(train_label), -1))\n",
    "    test_data = np.reshape(test_data, (len(test_label),-1))\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    clf = LogisticRegression(random_state=0, solver='liblinear',class_weight='balanced')\n",
    "    clf.fit(train_data, train_label)\n",
    "    #print (\" attack accuracy %.2f\" % (clf.score(test_data, test_label) * 100))\n",
    "    acc1 = balanced_accuracy_score(test_label,clf.predict(test_data))\n",
    "    #print (classification_report(test_label,clf.predict(test_feature)))\n",
    "    #auc1 = roc_auc_score(test_label,clf.predict(test_data))\n",
    "    f1_1 = f1_score(test_label,clf.predict(test_data), average='weighted')\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=100,max_depth=30, random_state=0,class_weight=\"balanced\")\n",
    "    \n",
    "    #from sklearn.svm import SVC\n",
    "    #clf = SVC(gamma='auto')\n",
    "    \n",
    "    #from sklearn.neural_network import MLPClassifier\n",
    "    #lf = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(10,5), random_state=1,max_iter=100)\n",
    "    \n",
    "    clf.fit(train_data, train_label)\n",
    "    acc2 = balanced_accuracy_score(test_label,clf.predict(test_data))\n",
    "    #auc2 = roc_auc_score(test_label,clf.predict(test_data))\n",
    "    f1_2 = f1_score(test_label,clf.predict(test_data), average='weighted')\n",
    "    #print (classification_report(test_label,mode.predict(test_feature)))\n",
    "    #print (\"balanced accuracy\",max(acc1,acc2))\n",
    "    #print (\"roc-auc\",max(auc1,auc2))\n",
    "    #print (\"f1 score\",max(f1_1,f1_2))\n",
    "    \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    clf = MLPClassifier(solver='sgd', alpha=0.01,hidden_layer_sizes=(50, len(unique_labels)), random_state=1)\n",
    "    clf.fit(train_data,train_label)\n",
    "    acc3 = balanced_accuracy_score(test_label,clf.predict(test_data))\n",
    "\n",
    "    return max(acc1,acc2,acc3)\n",
    "\n",
    "\n",
    "def all_analysis(epochs,name_prefix,dataset,model,eval_data_size,special_layers=None,num_layers=12,num_user=5,gap=0):\n",
    "    target_data_size = 5000\n",
    "    total_data_num = (num_user+1)*eval_data_size\n",
    "    all_epoch_cos = np.zeros((total_data_num,len(epochs),num_user))\n",
    "    all_epoch_loss = np.zeros((total_data_num,len(epochs),num_user))\n",
    "    all_epoch_target_loss = np.zeros((total_data_num,len(epochs),num_user))\n",
    "    all_epoch_grad_diff = np.zeros((total_data_num,len(epochs),num_user))\n",
    "    all_epoch_layer_cos = np.zeros((total_data_num,len(epochs),num_user,num_layers))\n",
    "    all_epoch_layer_grad_diff = np.zeros((total_data_num,len(epochs),num_user,num_layers))\n",
    "    all_epoch_target_after_loss = np.zeros((total_data_num,len(epochs),num_user))\n",
    "    all_epoch_label = np.zeros(total_data_num)\n",
    "    best_layer_dict = {'cifar100':6,'cifar10':2}\n",
    "    this_best_layer = best_layer_dict[dataset]\n",
    "    \n",
    "    for epoch_idx,epoch in enumerate(epochs):\n",
    "        data_name = prefix+'all_info_multi_party_member_attack_'+str(gap)+'_'+str(num_user)+'_0_0.0_0_0.0_0_'+str(epoch+1)+'_'+str(dataset)+'_'+str(target_data_size)+'_'+str(eval_data_size)+'_'+str(model)+'.npy'\n",
    "        data = np.load(data_name,allow_pickle=True)\n",
    "        label_name = prefix+'all_label_multi_party_member_attack_'+str(gap)+'_'+str(num_user)+'_0_0.0_0_0.0_0_'+str(epoch+1)+'_'+str(dataset)+'_'+str(target_data_size)+'_'+str(eval_data_size)+'_'+str(model)+'.npy'\n",
    "        label = np.load(label_name)\n",
    "        loss_data_name = prefix+'loss_info_multi_party_member_attack_'+str(gap)+'_'+str(num_user)+'_0_0.0_0_0.0_0_'+str(epoch+1)+'_'+str(dataset)+'_'+str(target_data_size)+'_'+str(eval_data_size)+'_'+str(model)+'.npy'\n",
    "        loss_data = np.load(loss_data_name,allow_pickle=True)\n",
    "        loss_label_name = prefix+'loss_label_multi_party_member_attack_'+str(gap)+'_'+str(num_user)+'_0_0.0_0_0.0_0_'+str(epoch+1)+'_'+str(dataset)+'_'+str(target_data_size)+'_'+str(eval_data_size)+'_'+str(model)+'.npy'\n",
    "        loss_label = np.load(loss_label_name,allow_pickle=True)\n",
    "        \n",
    "        target_loss_data_name = prefix+'target_model_before_loss_info_multi_party_member_attack_'+str(gap)+'_'+str(num_user)+'_0_0.0_0_0.0_0_'+str(epoch+1)+'_'+str(dataset)+'_'+str(target_data_size)+'_'+str(eval_data_size)+'_'+str(model)+'.npy'\n",
    "        target_loss_data = np.load(target_loss_data_name,allow_pickle=True)\n",
    "        target_after_loss_data_name = prefix+'target_model_after_loss_info_multi_party_member_attack_'+str(gap)+'_'+str(num_user)+'_0_0.0_0_0.0_0_'+str(epoch+1)+'_'+str(dataset)+'_'+str(target_data_size)+'_'+str(eval_data_size)+'_'+str(model)+'.npy'\n",
    "        target_after_loss_data = np.load(target_loss_data_name,allow_pickle=True)\n",
    "        data = np.reshape(data,(-1,num_user,12,14))\n",
    "        label = np.reshape(label,(-1))\n",
    "        loss_data = np.reshape(loss_data,(-1,num_user))\n",
    "        loss_label = np.reshape(loss_label,(-1))\n",
    "        target_loss_data = np.reshape(target_loss_data,(-1,1))\n",
    "        target_loss_data = np.tile(target_loss_data,(1,num_user))\n",
    "        target_after_loss_data = np.reshape(target_after_loss_data,(-1,1))\n",
    "        target_after_loss_data = np.tile(target_after_loss_data,(1,num_user))\n",
    "        #print (data.shape,label.shape,loss_data.shape,loss_label.shape,target_loss_data.shape)\n",
    "        \n",
    "        all_epoch_cos[:,epoch_idx,:] = data[:,:,this_best_layer,0]\n",
    "        all_epoch_grad_diff[:,epoch_idx,:] = data[:,:,this_best_layer,1] - data[:,:,this_best_layer,2]\n",
    "        all_epoch_layer_cos[:,epoch_idx,:,:] = data[:,:,:,0]\n",
    "        all_epoch_layer_grad_diff[:,epoch_idx,:,:] = data[:,:,:,1] - data[:,:,:,2]\n",
    "        all_epoch_loss[:,epoch_idx,:] = loss_data\n",
    "        all_epoch_target_loss[:,epoch_idx,:] = target_loss_data\n",
    "        all_epoch_target_after_loss[:,epoch_idx,:] = target_after_loss_data\n",
    "        all_epoch_label = label\n",
    "        \n",
    "        \n",
    "    all_epoch_analysis(all_epoch_cos,all_epoch_grad_diff,all_epoch_loss,all_epoch_target_loss,all_epoch_target_after_loss,all_epoch_label,member_index = num_user*eval_data_size)\n",
    "    #per_layer_analysis(all_epoch_cos,all_epoch_grad_diff,all_epoch_loss,all_epoch_target_loss,all_epoch_label,all_epoch_layer_cos,all_epoch_layer_grad_diff,member_index = num_user*eval_data_size)\n",
    "    #loss_analysis(all_epoch_cos,all_epoch_grad_diff,all_epoch_loss,all_epoch_target_loss,all_epoch_target_after_loss,all_epoch_label,member_index = num_user*eval_data_size)\n",
    "        \n",
    "        \n",
    "def all_epoch_analysis(all_epoch_cosine,all_epoch_grad_diff,all_epoch_loss,all_epoch_target_loss,all_epoch_target_after_loss,label,member_index):\n",
    "    ### for all epoch analysis and member-only case, we would like to see the following attacks:\n",
    "    # 1. sum of loss, argmin\n",
    "    # 2. sum of cosine, argmax\n",
    "    # 3. sum of grad-diff, argmax\n",
    "    # 4. all loss, NN\n",
    "    # 5. all cosine, NN\n",
    "    # 6. all grad-diff, NN\n",
    "    # 7. sum of loss, sum of cosine, sum of grad-diff, NN\n",
    "    # 8. all loss, all cosine, all grad-diff, NN\n",
    "    # 9. sum of sign of loss reduction, argmax\n",
    "    # 10. sum of loss, sum of cosine, sum of grad-diff, sum of sign of loss reduction, NN\n",
    "    import copy\n",
    "    num_instance = all_epoch_cosine.shape[0]\n",
    "    \n",
    "    #print (num_instance)\n",
    "    \n",
    "    max_acc = 0\n",
    "    print (\"member only case\")\n",
    "    #1\n",
    "    metric = np.sum(all_epoch_loss,axis=1)\n",
    "    this_acc = accuracy_score(np.argmin(metric[:member_index],axis=1),label[:member_index])\n",
    "    #print (f\"sum of loss acc {this_acc}\")\n",
    "    max_acc = max(max_acc,this_acc)\n",
    "\n",
    "    #2 \n",
    "    metric = np.sum(all_epoch_cosine,axis=1)\n",
    "    this_acc = accuracy_score(np.argmax(metric[:member_index],axis=1),label[:member_index])\n",
    "    #print (f\"sum of cosine acc {this_acc}\")\n",
    "    max_acc = max(max_acc,this_acc)\n",
    "\n",
    "    #3\n",
    "    metric = np.sum(all_epoch_grad_diff,axis=1)\n",
    "    this_acc = accuracy_score(np.argmax(metric[:member_index],axis=1),label[:member_index])\n",
    "    #print (f\"sum of grad diff acc {this_acc}\")\n",
    "    max_acc = max(max_acc,this_acc)\n",
    "\n",
    "    #11 grad_diff / loss\n",
    "    # p = e^(-L), grad multiplier = p - 1\n",
    "    multiplier = 1 - np.exp(-1*all_epoch_loss) \n",
    "    metric = np.sum(all_epoch_grad_diff/multiplier,axis=1)\n",
    "    this_acc = accuracy_score(np.argmax(metric[:member_index],axis=1),label[:member_index])\n",
    "    #print (f\"sum of grad_diff/loss acc {this_acc}\")\n",
    "    max_acc = max(max_acc,this_acc)\n",
    "\n",
    "    #4\n",
    "    metric = np.reshape(copy.deepcopy(all_epoch_loss),(num_instance,-1))\n",
    "    this_acc = report_acc(metric[:member_index],label[:member_index])\n",
    "    #print (f\"all epoch loss acc {this_acc}\")    \n",
    "    max_acc = max(max_acc,this_acc)\n",
    "\n",
    "    #5\n",
    "    metric = np.reshape(copy.deepcopy(all_epoch_cosine),(num_instance,-1))\n",
    "    this_acc = report_acc(metric[:member_index],label[:member_index])\n",
    "    #print (f\"all epoch cosine acc {this_acc}\")   \n",
    "    max_acc = max(max_acc,this_acc)\n",
    "\n",
    "    #6\n",
    "    metric = np.reshape(copy.deepcopy(all_epoch_grad_diff),(num_instance,-1))\n",
    "    this_acc = report_acc(metric[:member_index],label[:member_index])\n",
    "    #print (f\"all epoch grad diff acc {this_acc}\")\n",
    "    max_acc = max(max_acc,this_acc)\n",
    "    \n",
    "    #7\n",
    "    metric = np.concatenate((np.sum(all_epoch_loss,axis=1),\n",
    "                             np.sum(all_epoch_cosine,axis=1),\n",
    "                             np.sum(all_epoch_grad_diff,axis=1)),axis=1)\n",
    "    this_acc = report_acc(metric[:member_index],label[:member_index])\n",
    "    #print (f\"all epoch sum acc {this_acc}\")\n",
    "    max_acc = max(max_acc,this_acc)\n",
    "    \n",
    "    #8\n",
    "    metric = np.concatenate((np.reshape(copy.deepcopy(all_epoch_loss),(num_instance,-1)),\n",
    "                             np.reshape(copy.deepcopy(all_epoch_cosine),(num_instance,-1)),\n",
    "                             np.reshape(copy.deepcopy(all_epoch_grad_diff),(num_instance,-1))),axis=1)\n",
    "    this_acc = report_acc(metric[:member_index],label[:member_index])\n",
    "    #print (f\"all epoch loss+cosine+grad_diff acc {this_acc}\")\n",
    "    max_acc = max(max_acc,this_acc)\n",
    "    \n",
    "    #9\n",
    "    loss_reduction = all_epoch_target_after_loss - all_epoch_loss\n",
    "    loss_reduction = np.reshape(copy.deepcopy(loss_reduction),(num_instance,-1))\n",
    "    #loss_reduction_sign_metric = np.sum(sign_loss_reduction,axis=1)\n",
    "    #print (loss_reduction_sign_metric)\n",
    "    metric = loss_reduction\n",
    "    this_acc = report_acc(metric[:member_index],label[:member_index])\n",
    "    #print (f\"sum of loss reduction acc {this_acc}\")\n",
    "    max_acc = max(max_acc,this_acc)\n",
    "    \n",
    "    #10\n",
    "    metric = np.concatenate((np.reshape(copy.deepcopy(all_epoch_loss),(num_instance,-1)),\n",
    "                             np.reshape(copy.deepcopy(all_epoch_cosine),(num_instance,-1)),\n",
    "                             np.reshape(copy.deepcopy(all_epoch_grad_diff),(num_instance,-1)),\n",
    "                            np.reshape(copy.deepcopy(loss_reduction),(num_instance,-1))),axis=1)\n",
    "    this_acc = report_acc(metric[:member_index],label[:member_index])\n",
    "    #print (f\"all epoch loss+cosine+grad_diff+loss_reduction acc {this_acc}\")\n",
    "    max_acc = max(max_acc,this_acc)\n",
    "    \n",
    "    print (f\"MAXIMUM ACC {max_acc}\")\n",
    "    \n",
    "    max_acc = 0\n",
    "    ### for non-member including case, we need to consider the following attacks:\n",
    "    #1. sum loss \n",
    "    #2. all loss\n",
    "    #3. sum cos\n",
    "    #4. all cos\n",
    "    #5. sum grad diff\n",
    "    #6. all grad diff\n",
    "    #7. all feature\n",
    "    print (\"testing instances included\")\n",
    "    #1\n",
    "    metric = np.sum(all_epoch_loss,axis=1)\n",
    "    this_acc = report_acc(metric,label)\n",
    "    #print (f\"sum of loss acc {this_acc}\")\n",
    "    max_acc = max(this_acc,max_acc)\n",
    "    \n",
    "    #2 \n",
    "    metric = np.sum(all_epoch_cosine,axis=1)\n",
    "    this_acc = report_acc(metric,label)\n",
    "    #print (f\"sum of cosine acc {this_acc}\")\n",
    "    max_acc = max(this_acc,max_acc)\n",
    "    \n",
    "    #3\n",
    "    metric = np.sum(all_epoch_grad_diff,axis=1)\n",
    "    this_acc = report_acc(metric,label)\n",
    "    #print (f\"sum of grad diff acc {this_acc}\")\n",
    "    max_acc = max(this_acc,max_acc)\n",
    "    \n",
    "    #4\n",
    "    metric = np.reshape(copy.deepcopy(all_epoch_loss),(num_instance,-1))\n",
    "    this_acc = report_acc(metric,label)\n",
    "    #print (f\"all epoch loss acc {this_acc}\")   \n",
    "    max_acc = max(this_acc,max_acc) \n",
    "    \n",
    "    #5\n",
    "    metric = np.reshape(copy.deepcopy(all_epoch_cosine),(num_instance,-1))\n",
    "    this_acc = report_acc(metric,label)\n",
    "    #print (f\"all epoch cosine acc {this_acc}\")   \n",
    "    max_acc = max(this_acc,max_acc)\n",
    "    \n",
    "    #6\n",
    "    metric = np.reshape(copy.deepcopy(all_epoch_grad_diff),(num_instance,-1))\n",
    "    this_acc = report_acc(metric,label)\n",
    "    #print (f\"all epoch grad diff acc {this_acc}\")\n",
    "    max_acc = max(this_acc,max_acc)\n",
    "    \n",
    "    #7\n",
    "    metric = np.concatenate((np.sum(all_epoch_loss,axis=1),\n",
    "                             np.sum(all_epoch_cosine,axis=1),\n",
    "                             np.sum(all_epoch_grad_diff,axis=1)),axis=1)\n",
    "    this_acc = report_acc(metric,label)\n",
    "    #print (f\"all epoch sum acc {this_acc}\")\n",
    "    max_acc = max(this_acc,max_acc)\n",
    "    \n",
    "    #8\n",
    "    metric = np.concatenate((np.reshape(copy.deepcopy(all_epoch_loss),(num_instance,-1)),\n",
    "                             np.reshape(copy.deepcopy(all_epoch_cosine),(num_instance,-1)),\n",
    "                             np.reshape(copy.deepcopy(all_epoch_grad_diff),(num_instance,-1))),axis=1)\n",
    "    this_acc = report_acc(metric,label)\n",
    "    #print (f\"all epoch loss+cosine+grad_diff acc {this_acc}\")\n",
    "    max_acc = max(this_acc,max_acc)\n",
    "    \n",
    "    #9\n",
    "    loss_reduction = all_epoch_target_after_loss - all_epoch_loss\n",
    "    loss_reduction = np.reshape(copy.deepcopy(loss_reduction),(num_instance,-1))\n",
    "    #loss_reduction_sign_metric = np.sum(sign_loss_reduction,axis=1)\n",
    "    #print (loss_reduction_sign_metric)\n",
    "    metric = loss_reduction\n",
    "    this_acc = report_acc(metric,label)\n",
    "    #print (f\"sum of loss reduction acc {this_acc}\")\n",
    "    max_acc = max(this_acc,max_acc)\n",
    "\n",
    "    #10\n",
    "    metric = np.concatenate((np.reshape(copy.deepcopy(all_epoch_loss),(num_instance,-1)),\n",
    "                             np.reshape(copy.deepcopy(all_epoch_cosine),(num_instance,-1)),\n",
    "                             np.reshape(copy.deepcopy(all_epoch_grad_diff),(num_instance,-1)),\n",
    "                            np.reshape(copy.deepcopy(loss_reduction),(num_instance,-1))),axis=1)\n",
    "    this_acc = report_acc(metric,label)\n",
    "    #print (f\"all epoch loss+cosine+grad_diff+loss_reduction acc {this_acc}\")\n",
    "    max_acc = max(this_acc,max_acc)\n",
    "    print (f\"MAXIMUM ACC {max_acc}\")\n",
    "    \n",
    "\n",
    "def per_layer_analysis(all_epoch_cosine,all_epoch_grad_diff,all_epoch_loss,all_epoch_target_loss,label,all_epoch_layer_cosine,all_epoch_layer_grad_diff,member_index):\n",
    "    ### for per layer analysis, we want to see the difference between 1. loss 2. csoine 3. grad_diff for member only case\n",
    "    \n",
    "    num_layers = all_epoch_layer_cosine.shape[-1]\n",
    "    num_instance = all_epoch_cosine.shape[0]\n",
    "    num_epoch = all_epoch_cosine.shape[1]\n",
    "    num_user = all_epoch_cosine.shape[2]    \n",
    "    eval_data_size = 5000\n",
    "    \n",
    "    print (\"member only case\")\n",
    "    \n",
    "    all_epoch_loss_acc = []\n",
    "    all_epoch_cos_acc = []\n",
    "    all_epoch_grad_diff_acc = []\n",
    "    \n",
    "    for epoch_idx in range(num_epoch):\n",
    "        #print (f\"epoch:{epoch_idx*10}\")\n",
    "        layer_cosine_acc = []\n",
    "        layer_grad_diff_acc = []\n",
    "        this_epoch_loss = all_epoch_loss[:,epoch_idx,:]\n",
    "        for layer_idx in range(num_layers):\n",
    "            this_layer_cosine = all_epoch_layer_cosine[:,epoch_idx,:,layer_idx]\n",
    "            this_layer_grad_diff = all_epoch_layer_grad_diff[:,epoch_idx,:,layer_idx]\n",
    "            layer_cosine_acc.append(accuracy_score(np.argmax(this_layer_cosine,axis=1)[:member_index],label[:member_index]))\n",
    "            layer_grad_diff_acc.append(accuracy_score(np.argmax(this_layer_grad_diff,axis=1)[:member_index],label[:member_index]))\n",
    "        loss_acc = accuracy_score(np.argmin(this_epoch_loss,axis=1)[:member_index],label[:member_index])\n",
    "        \n",
    "        #fig = plt.figure(figsize=(5,5))\n",
    "        #plt.plot(np.arange(num_layers),layer_cosine_acc)\n",
    "        #plt.plot(np.arange(num_layers),layer_grad_diff_acc)\n",
    "        #plt.plot(np.arange(num_layers),[loss_acc for i in range(num_layers)])\n",
    "        #plt.legend(['cosine','grad-diff','loss'])\n",
    "        #plt.show()\n",
    "        \n",
    "        all_epoch_loss_acc.append(loss_acc)\n",
    "        all_epoch_cos_acc.append(max(layer_cosine_acc))\n",
    "        all_epoch_grad_diff_acc.append(max(layer_grad_diff_acc))\n",
    "                                           \n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.plot(np.arange(num_epoch)*10,all_epoch_cos_acc)\n",
    "    plt.plot(np.arange(num_epoch)*10,all_epoch_grad_diff_acc)\n",
    "    plt.plot(np.arange(num_epoch)*10,all_epoch_loss_acc)\n",
    "    plt.legend(['cosine','grad-diff','loss'])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print (\"testing instances included\")\n",
    "    \n",
    "    all_epoch_loss_acc = []\n",
    "    all_epoch_cos_acc = []\n",
    "    all_epoch_grad_diff_acc = []\n",
    "    \n",
    "    for epoch_idx in range(num_epoch):\n",
    "        #print (f\"epoch:{epoch_idx*10}\")\n",
    "        layer_cosine_acc = []\n",
    "        layer_grad_diff_acc = []\n",
    "        this_epoch_loss = all_epoch_loss[:,epoch_idx,:]\n",
    "        for layer_idx in range(num_layers):\n",
    "            this_layer_cosine = all_epoch_layer_cosine[:,epoch_idx,:,layer_idx]\n",
    "            this_layer_grad_diff = all_epoch_layer_grad_diff[:,epoch_idx,:,layer_idx]\n",
    "            layer_cosine_acc.append(report_acc(this_layer_cosine,label))\n",
    "            layer_grad_diff_acc.append(report_acc(this_layer_grad_diff,label))\n",
    "        loss_acc = report_acc(this_epoch_loss,label)\n",
    "        \n",
    "        #fig = plt.figure(figsize=(5,5))\n",
    "        #plt.plot(np.arange(num_layers),layer_cosine_acc)\n",
    "        #plt.plot(np.arange(num_layers),layer_grad_diff_acc)\n",
    "        #plt.plot(np.arange(num_layers),[loss_acc for i in range(num_layers)])\n",
    "        #plt.legend(['cosine','grad-diff','loss'])\n",
    "        #plt.show()\n",
    "        \n",
    "        all_epoch_loss_acc.append(loss_acc)\n",
    "        all_epoch_cos_acc.append(max(layer_cosine_acc))\n",
    "        all_epoch_grad_diff_acc.append(max(layer_grad_diff_acc))\n",
    "                                           \n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.plot(np.arange(num_epoch)*10,all_epoch_cos_acc)\n",
    "    plt.plot(np.arange(num_epoch)*10,all_epoch_grad_diff_acc)\n",
    "    plt.plot(np.arange(num_epoch)*10,all_epoch_loss_acc)\n",
    "    plt.legend(['cosine','grad-diff','loss'])\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "def loss_analysis(all_epoch_cosine,all_epoch_grad_diff,all_epoch_loss,all_epoch_target_loss,all_epoch_target_after_loss,label,member_index):\n",
    "    eval_data_size = 5000\n",
    "    num_instance = all_epoch_cosine.shape[0]\n",
    "    num_epoch = all_epoch_cosine.shape[1]\n",
    "    num_user = all_epoch_cosine.shape[2]\n",
    "    \n",
    "    ### voting attack\n",
    "    loss_reduction = all_epoch_target_loss - all_epoch_loss\n",
    "    sign_loss_reduction = np.sign(loss_reduction)\n",
    "    #sign_loss_reduction = sign_loss_reduction[:,13:17,:]\n",
    "    all_voting_pred = np.argmax(np.sum(sign_loss_reduction[:,:,:],axis=1),axis=1)\n",
    "    #print (f\"member-only case: loss reduction sign attack acc{accuracy_score(all_voting_pred[:member_index],label[:member_index])}\")\n",
    "    \n",
    "    #fig = plt.figure(figsize=(8,8))\n",
    "    #all_sign = []\n",
    "    #for i in range(num_epoch):\n",
    "    #    this_user_dis = np.sum(np.maximum(sign_loss_reduction[:eval_data_size,i,:],0),axis=0)\n",
    "        #print (this_user_dis)\n",
    "    #    all_sign.append(this_user_dis)\n",
    "    #all_sign = np.array(all_sign)\n",
    "    #plt.plot(np.arange(num_epoch),all_sign[:,0])\n",
    "    #plt.plot(np.arange(num_epoch),all_sign[:,1])\n",
    "    #plt.plot(np.arange(num_epoch),all_sign[:,2]) \n",
    "    #plt.legend(['1','2','3'])\n",
    "    #plt.show()\n",
    "    \n",
    "    target_loss = []\n",
    "    user_loss = [[] for i in range(num_user)]\n",
    "    for i in range(num_epoch):\n",
    "        target_loss.append([np.average(all_epoch_target_after_loss[:eval_data_size,i,:]),\n",
    "                            np.std(all_epoch_target_after_loss[:eval_data_size,i,:])])\n",
    "        for j in range(num_user):\n",
    "            user_loss[j].append([np.average(all_epoch_loss[:eval_data_size,i,j]),\n",
    "                                 np.std(all_epoch_loss[:eval_data_size,i,j])])\n",
    "    \n",
    "    user_loss = np.array(user_loss)\n",
    "    target_loss = np.array(target_loss)\n",
    "    #print (target_loss.shape,user_loss.shape)\n",
    "                             \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.arange(num_epoch),user_loss[0,:,0])\n",
    "    plt.plot(np.arange(num_epoch),user_loss[1,:,0])\n",
    "    plt.plot(np.arange(num_epoch),user_loss[2,:,0]) \n",
    "    plt.plot(np.arange(num_epoch),target_loss[:,0])\n",
    "    plt.legend(['1','2','3','target'])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.errorbar(np.arange(num_epoch),user_loss[0,:,0],user_loss[0,:,1])\n",
    "    plt.errorbar(np.arange(num_epoch),user_loss[1,:,0],user_loss[1,:,1])\n",
    "    plt.errorbar(np.arange(num_epoch),user_loss[2,:,0],user_loss[2,:,1]) \n",
    "    plt.errorbar(np.arange(num_epoch),target_loss[:,0],target_loss[:,1])\n",
    "    plt.legend(['1','2','3','target'])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #sp_voting = np.sum(sign_loss_reduction[:eval_data_size],axis=1).astype(np.int64)\n",
    "    #for i in range(num_user):\n",
    "    #    #plt.subplot(1,3,i+1)\n",
    "    #    this_user_dis = sp_voting[:,i]\n",
    "    #    counts = np.bincount(this_user_dis+30)/eval_data_size\n",
    "    #    #print (counts.shape)\n",
    "    #    plt.plot(np.arange(len(counts))-30,counts)\n",
    "    #plt.legend(['1','2','3'])\n",
    "    #plt.show()\n",
    "    \n",
    "    '''\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    sp_voting = np.sum(sign_loss_reduction[-eval_data_size:],axis=1).astype(np.int64)\n",
    "    for i in range(num_user):\n",
    "        #plt.subplot(1,3,i+1)\n",
    "        this_user_dis = sp_voting[:,i]\n",
    "        counts = np.bincount(this_user_dis+30)/eval_data_size\n",
    "        #print (counts.shape)\n",
    "        plt.plot(np.arange(len(counts))-30,counts)\n",
    "    plt.legend(['1','2','3'])\n",
    "    plt.show()\n",
    "    '''\n",
    "    ## show the loss distribution for all epochs\n",
    "    #member_loss = []\n",
    "    #nonmember_loss = []\n",
    "    #for epoch_idx in range(num_epoch):\n",
    "        ## calculate the sign this epoch for member and non-member for user 1\n",
    "    #    this_user_member_index = np.arange(5000)\n",
    "    #    this_user_nonmember_index = np.arange(num_instance)[-5000:]\n",
    "        \n",
    "    #    this_epoch_member_loss = all_epoch_loss[this_user_member_index,epoch_idx,0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active attacker gradien ascent every 2 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.58\n",
      "sum of cosine acc 0.5\n",
      "sum of grad diff acc 0.5033333333333333\n",
      "sum of grad_diff/loss acc 0.5233333333333333\n",
      "all epoch loss acc 0.6\n",
      "all epoch cosine acc 0.56\n",
      "all epoch grad diff acc 0.5133333333333333\n",
      "all epoch sum acc 0.5533333333333333\n",
      "all epoch loss+cosine+grad_diff acc 0.49333333333333335\n",
      "sum of loss reduction acc 0.5533333333333333\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.5466666666666666\n",
      "MAXIMUM ACC 0.6\n",
      "testing instances included\n",
      "sum of loss acc 0.42000000000000004\n",
      "sum of cosine acc 0.41000000000000003\n",
      "sum of grad diff acc 0.45\n",
      "all epoch loss acc 0.47\n",
      "all epoch cosine acc 0.455\n",
      "all epoch grad diff acc 0.43500000000000005\n",
      "all epoch sum acc 0.45\n",
      "all epoch loss+cosine+grad_diff acc 0.435\n",
      "sum of loss reduction acc 0.42500000000000004\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.46499999999999997\n",
      "MAXIMUM ACC 0.47\n",
      "active attacker gradien ascent every 3 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.5633333333333334\n",
      "sum of cosine acc 0.4633333333333333\n",
      "sum of grad diff acc 0.5033333333333333\n",
      "sum of grad_diff/loss acc 0.4766666666666667\n",
      "all epoch loss acc 0.5466666666666667\n",
      "all epoch cosine acc 0.44\n",
      "all epoch grad diff acc 0.4733333333333334\n",
      "all epoch sum acc 0.5733333333333334\n",
      "all epoch loss+cosine+grad_diff acc 0.52\n",
      "sum of loss reduction acc 0.5\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.5533333333333333\n",
      "MAXIMUM ACC 0.5733333333333334\n",
      "testing instances included\n",
      "sum of loss acc 0.45000000000000007\n",
      "sum of cosine acc 0.36\n",
      "sum of grad diff acc 0.35\n",
      "all epoch loss acc 0.46\n",
      "all epoch cosine acc 0.33499999999999996\n",
      "all epoch grad diff acc 0.325\n",
      "all epoch sum acc 0.41\n",
      "all epoch loss+cosine+grad_diff acc 0.45999999999999996\n",
      "sum of loss reduction acc 0.415\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.46\n",
      "MAXIMUM ACC 0.46\n",
      "active attacker gradien ascent every 4 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.51\n",
      "sum of cosine acc 0.5\n",
      "sum of grad diff acc 0.47\n",
      "sum of grad_diff/loss acc 0.49333333333333335\n",
      "all epoch loss acc 0.5599999999999999\n",
      "all epoch cosine acc 0.48\n",
      "all epoch grad diff acc 0.4666666666666666\n",
      "all epoch sum acc 0.5066666666666667\n",
      "all epoch loss+cosine+grad_diff acc 0.48\n",
      "sum of loss reduction acc 0.5133333333333333\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.5\n",
      "MAXIMUM ACC 0.5599999999999999\n",
      "testing instances included\n",
      "sum of loss acc 0.43\n",
      "sum of cosine acc 0.37500000000000006\n",
      "sum of grad diff acc 0.35\n",
      "all epoch loss acc 0.425\n",
      "all epoch cosine acc 0.43\n",
      "all epoch grad diff acc 0.405\n",
      "all epoch sum acc 0.37\n",
      "all epoch loss+cosine+grad_diff acc 0.37\n",
      "sum of loss reduction acc 0.43\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.43499999999999994\n",
      "MAXIMUM ACC 0.43499999999999994\n",
      "active attacker gradien ascent every 5 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.51\n",
      "sum of cosine acc 0.46\n",
      "sum of grad diff acc 0.5033333333333333\n",
      "sum of grad_diff/loss acc 0.5133333333333333\n",
      "all epoch loss acc 0.52\n",
      "all epoch cosine acc 0.5133333333333333\n",
      "all epoch grad diff acc 0.47333333333333333\n",
      "all epoch sum acc 0.5533333333333333\n",
      "all epoch loss+cosine+grad_diff acc 0.5133333333333333\n",
      "sum of loss reduction acc 0.45333333333333337\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.45999999999999996\n",
      "MAXIMUM ACC 0.5533333333333333\n",
      "testing instances included\n",
      "sum of loss acc 0.42500000000000004\n",
      "sum of cosine acc 0.39499999999999996\n",
      "sum of grad diff acc 0.38\n",
      "all epoch loss acc 0.36\n",
      "all epoch cosine acc 0.34500000000000003\n",
      "all epoch grad diff acc 0.395\n",
      "all epoch sum acc 0.44499999999999995\n",
      "all epoch loss+cosine+grad_diff acc 0.43499999999999994\n",
      "sum of loss reduction acc 0.375\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.35000000000000003\n",
      "MAXIMUM ACC 0.44499999999999995\n",
      "active attacker gradien ascent every 6 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.47\n",
      "sum of cosine acc 0.43\n",
      "sum of grad diff acc 0.48\n",
      "sum of grad_diff/loss acc 0.47333333333333333\n",
      "all epoch loss acc 0.5333333333333333\n",
      "all epoch cosine acc 0.42\n",
      "all epoch grad diff acc 0.48666666666666664\n",
      "all epoch sum acc 0.47333333333333333\n",
      "all epoch loss+cosine+grad_diff acc 0.5333333333333333\n",
      "sum of loss reduction acc 0.5133333333333333\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.45999999999999996\n",
      "MAXIMUM ACC 0.5333333333333333\n",
      "testing instances included\n",
      "sum of loss acc 0.355\n",
      "sum of cosine acc 0.325\n",
      "sum of grad diff acc 0.34500000000000003\n",
      "all epoch loss acc 0.38\n",
      "all epoch cosine acc 0.32000000000000006\n",
      "all epoch grad diff acc 0.32\n",
      "all epoch sum acc 0.34500000000000003\n",
      "all epoch loss+cosine+grad_diff acc 0.34500000000000003\n",
      "sum of loss reduction acc 0.39499999999999996\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.335\n",
      "MAXIMUM ACC 0.39499999999999996\n",
      "active attacker gradien ascent every 7 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.49\n",
      "sum of cosine acc 0.43666666666666665\n",
      "sum of grad diff acc 0.4633333333333333\n",
      "sum of grad_diff/loss acc 0.4633333333333333\n",
      "all epoch loss acc 0.45333333333333337\n",
      "all epoch cosine acc 0.4266666666666667\n",
      "all epoch grad diff acc 0.42\n",
      "all epoch sum acc 0.5466666666666667\n",
      "all epoch loss+cosine+grad_diff acc 0.47333333333333333\n",
      "sum of loss reduction acc 0.49333333333333335\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.48666666666666664\n",
      "MAXIMUM ACC 0.5466666666666667\n",
      "testing instances included\n",
      "sum of loss acc 0.36000000000000004\n",
      "sum of cosine acc 0.32500000000000007\n",
      "sum of grad diff acc 0.35\n",
      "all epoch loss acc 0.42000000000000004\n",
      "all epoch cosine acc 0.32\n",
      "all epoch grad diff acc 0.33\n",
      "all epoch sum acc 0.395\n",
      "all epoch loss+cosine+grad_diff acc 0.36500000000000005\n",
      "sum of loss reduction acc 0.29500000000000004\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.37\n",
      "MAXIMUM ACC 0.42000000000000004\n",
      "active attacker gradien ascent every 8 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.49666666666666665\n",
      "sum of cosine acc 0.42333333333333334\n",
      "sum of grad diff acc 0.49666666666666665\n",
      "sum of grad_diff/loss acc 0.4866666666666667\n",
      "all epoch loss acc 0.42\n",
      "all epoch cosine acc 0.48666666666666664\n",
      "all epoch grad diff acc 0.44\n",
      "all epoch sum acc 0.4666666666666666\n",
      "all epoch loss+cosine+grad_diff acc 0.39999999999999997\n",
      "sum of loss reduction acc 0.49333333333333335\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.48666666666666664\n",
      "MAXIMUM ACC 0.49666666666666665\n",
      "testing instances included\n",
      "sum of loss acc 0.43500000000000005\n",
      "sum of cosine acc 0.265\n",
      "sum of grad diff acc 0.325\n",
      "all epoch loss acc 0.35\n",
      "all epoch cosine acc 0.29500000000000004\n",
      "all epoch grad diff acc 0.33499999999999996\n",
      "all epoch sum acc 0.365\n",
      "all epoch loss+cosine+grad_diff acc 0.31500000000000006\n",
      "sum of loss reduction acc 0.32\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.33\n",
      "MAXIMUM ACC 0.43500000000000005\n",
      "active attacker gradien ascent every 9 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.5066666666666667\n",
      "sum of cosine acc 0.4666666666666667\n",
      "sum of grad diff acc 0.46\n",
      "sum of grad_diff/loss acc 0.48\n",
      "all epoch loss acc 0.4533333333333333\n",
      "all epoch cosine acc 0.45999999999999996\n",
      "all epoch grad diff acc 0.38000000000000006\n",
      "all epoch sum acc 0.4466666666666667\n",
      "all epoch loss+cosine+grad_diff acc 0.43999999999999995\n",
      "sum of loss reduction acc 0.5066666666666667\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.43333333333333335\n",
      "MAXIMUM ACC 0.5066666666666667\n",
      "testing instances included\n",
      "sum of loss acc 0.375\n",
      "sum of cosine acc 0.30999999999999994\n",
      "sum of grad diff acc 0.34500000000000003\n",
      "all epoch loss acc 0.36500000000000005\n",
      "all epoch cosine acc 0.33\n",
      "all epoch grad diff acc 0.305\n",
      "all epoch sum acc 0.405\n",
      "all epoch loss+cosine+grad_diff acc 0.36000000000000004\n",
      "sum of loss reduction acc 0.35000000000000003\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.33999999999999997\n",
      "MAXIMUM ACC 0.405\n",
      "active attacker gradien ascent every 10 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.5066666666666667\n",
      "sum of cosine acc 0.44333333333333336\n",
      "sum of grad diff acc 0.42333333333333334\n",
      "sum of grad_diff/loss acc 0.45666666666666667\n",
      "all epoch loss acc 0.4533333333333333\n",
      "all epoch cosine acc 0.43333333333333335\n",
      "all epoch grad diff acc 0.39999999999999997\n",
      "all epoch sum acc 0.48666666666666664\n",
      "all epoch loss+cosine+grad_diff acc 0.4466666666666666\n",
      "sum of loss reduction acc 0.48666666666666664\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.47333333333333333\n",
      "MAXIMUM ACC 0.5066666666666667\n",
      "testing instances included\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of loss acc 0.415\n",
      "sum of cosine acc 0.32\n",
      "sum of grad diff acc 0.31500000000000006\n",
      "all epoch loss acc 0.365\n",
      "all epoch cosine acc 0.335\n",
      "all epoch grad diff acc 0.31\n",
      "all epoch sum acc 0.39\n",
      "all epoch loss+cosine+grad_diff acc 0.345\n",
      "sum of loss reduction acc 0.38\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.37\n",
      "MAXIMUM ACC 0.415\n"
     ]
    }
   ],
   "source": [
    "gap = np.array([2,3,4,5,6,7,8,9,10])\n",
    "prefix = '/Users/jclialex/PycharmProjects/whiteboxmi_expdata/expdata/expdata/'\n",
    "epochs = ((np.arange(29)+1)*10)\n",
    "\n",
    "for this_gap in gap:\n",
    "    print (f\"active attacker gradien ascent every {this_gap} epoch\")\n",
    "    all_analysis(epochs,prefix,'cifar10','alexnet',100,num_user=3,gap=this_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active attacker gradien ascent every 2 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.465\n",
      "sum of cosine acc 0.41\n",
      "sum of grad diff acc 0.425\n",
      "sum of grad_diff/loss acc 0.4066666666666667\n",
      "all epoch loss acc 0.5166666666666667\n",
      "all epoch cosine acc 0.4366666666666667\n",
      "all epoch grad diff acc 0.43333333333333335\n",
      "all epoch sum acc 0.5933333333333334\n",
      "all epoch loss+cosine+grad_diff acc 0.5666666666666668\n",
      "sum of loss reduction acc 0.47333333333333333\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.5533333333333333\n",
      "MAXIMUM ACC 0.5933333333333334\n",
      "testing instances included\n",
      "sum of loss acc 0.45\n",
      "sum of cosine acc 0.3425\n",
      "sum of grad diff acc 0.3675\n",
      "all epoch loss acc 0.4575\n",
      "all epoch cosine acc 0.38749999999999996\n",
      "all epoch grad diff acc 0.34249999999999997\n",
      "all epoch sum acc 0.475\n",
      "all epoch loss+cosine+grad_diff acc 0.4375\n",
      "sum of loss reduction acc 0.44000000000000006\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.45749999999999996\n",
      "MAXIMUM ACC 0.475\n",
      "active attacker gradien ascent every 3 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.46\n",
      "sum of cosine acc 0.4066666666666667\n",
      "sum of grad diff acc 0.41\n",
      "sum of grad_diff/loss acc 0.395\n",
      "all epoch loss acc 0.5266666666666667\n",
      "all epoch cosine acc 0.38999999999999996\n",
      "all epoch grad diff acc 0.43333333333333335\n",
      "all epoch sum acc 0.5166666666666666\n",
      "all epoch loss+cosine+grad_diff acc 0.52\n",
      "sum of loss reduction acc 0.5\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.51\n",
      "MAXIMUM ACC 0.5266666666666667\n",
      "testing instances included\n",
      "sum of loss acc 0.39999999999999997\n",
      "sum of cosine acc 0.29000000000000004\n",
      "sum of grad diff acc 0.36\n",
      "all epoch loss acc 0.37\n",
      "all epoch cosine acc 0.34500000000000003\n",
      "all epoch grad diff acc 0.3425\n",
      "all epoch sum acc 0.41500000000000004\n",
      "all epoch loss+cosine+grad_diff acc 0.4125\n",
      "sum of loss reduction acc 0.38\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.38749999999999996\n",
      "MAXIMUM ACC 0.41500000000000004\n",
      "active attacker gradien ascent every 4 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.44666666666666666\n",
      "sum of cosine acc 0.3983333333333333\n",
      "sum of grad diff acc 0.42333333333333334\n",
      "sum of grad_diff/loss acc 0.3983333333333333\n",
      "all epoch loss acc 0.4266666666666667\n",
      "all epoch cosine acc 0.36999999999999994\n",
      "all epoch grad diff acc 0.4166666666666667\n",
      "all epoch sum acc 0.4533333333333333\n",
      "all epoch loss+cosine+grad_diff acc 0.48999999999999994\n",
      "sum of loss reduction acc 0.4366666666666667\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.4166666666666667\n",
      "MAXIMUM ACC 0.48999999999999994\n",
      "testing instances included\n",
      "sum of loss acc 0.35500000000000004\n",
      "sum of cosine acc 0.28750000000000003\n",
      "sum of grad diff acc 0.32999999999999996\n",
      "all epoch loss acc 0.3825\n",
      "all epoch cosine acc 0.285\n",
      "all epoch grad diff acc 0.3075\n",
      "all epoch sum acc 0.36\n",
      "all epoch loss+cosine+grad_diff acc 0.35749999999999993\n",
      "sum of loss reduction acc 0.3675\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.3275\n",
      "MAXIMUM ACC 0.3825\n",
      "active attacker gradien ascent every 5 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.43\n",
      "sum of cosine acc 0.39\n",
      "sum of grad diff acc 0.405\n",
      "sum of grad_diff/loss acc 0.4116666666666667\n",
      "all epoch loss acc 0.45333333333333337\n",
      "all epoch cosine acc 0.39999999999999997\n",
      "all epoch grad diff acc 0.42333333333333334\n",
      "all epoch sum acc 0.4766666666666666\n",
      "all epoch loss+cosine+grad_diff acc 0.43333333333333335\n",
      "sum of loss reduction acc 0.47333333333333333\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.42\n",
      "MAXIMUM ACC 0.4766666666666666\n",
      "testing instances included\n",
      "sum of loss acc 0.31000000000000005\n",
      "sum of cosine acc 0.2975\n",
      "sum of grad diff acc 0.3175\n",
      "all epoch loss acc 0.34500000000000003\n",
      "all epoch cosine acc 0.3125\n",
      "all epoch grad diff acc 0.31\n",
      "all epoch sum acc 0.34\n",
      "all epoch loss+cosine+grad_diff acc 0.36\n",
      "sum of loss reduction acc 0.33\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.33\n",
      "MAXIMUM ACC 0.36\n",
      "active attacker gradien ascent every 6 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.42333333333333334\n",
      "sum of cosine acc 0.365\n",
      "sum of grad diff acc 0.37333333333333335\n",
      "sum of grad_diff/loss acc 0.37833333333333335\n",
      "all epoch loss acc 0.41999999999999993\n",
      "all epoch cosine acc 0.3733333333333333\n",
      "all epoch grad diff acc 0.33666666666666667\n",
      "all epoch sum acc 0.44333333333333336\n",
      "all epoch loss+cosine+grad_diff acc 0.3933333333333333\n",
      "sum of loss reduction acc 0.4366666666666667\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.38999999999999996\n",
      "MAXIMUM ACC 0.44333333333333336\n",
      "testing instances included\n",
      "sum of loss acc 0.2975\n",
      "sum of cosine acc 0.2825\n",
      "sum of grad diff acc 0.31\n",
      "all epoch loss acc 0.3275\n",
      "all epoch cosine acc 0.28250000000000003\n",
      "all epoch grad diff acc 0.29000000000000004\n",
      "all epoch sum acc 0.3525\n",
      "all epoch loss+cosine+grad_diff acc 0.28750000000000003\n",
      "sum of loss reduction acc 0.3075\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.29000000000000004\n",
      "MAXIMUM ACC 0.3525\n",
      "active attacker gradien ascent every 7 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.45\n",
      "sum of cosine acc 0.36\n",
      "sum of grad diff acc 0.37833333333333335\n",
      "sum of grad_diff/loss acc 0.385\n",
      "all epoch loss acc 0.4000000000000001\n",
      "all epoch cosine acc 0.3933333333333333\n",
      "all epoch grad diff acc 0.38999999999999996\n",
      "all epoch sum acc 0.4166666666666667\n",
      "all epoch loss+cosine+grad_diff acc 0.3666666666666667\n",
      "sum of loss reduction acc 0.3666666666666667\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.37333333333333335\n",
      "MAXIMUM ACC 0.45\n",
      "testing instances included\n",
      "sum of loss acc 0.31\n",
      "sum of cosine acc 0.29000000000000004\n",
      "sum of grad diff acc 0.2925\n",
      "all epoch loss acc 0.3225\n",
      "all epoch cosine acc 0.325\n",
      "all epoch grad diff acc 0.275\n",
      "all epoch sum acc 0.325\n",
      "all epoch loss+cosine+grad_diff acc 0.29000000000000004\n",
      "sum of loss reduction acc 0.3075\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.3075\n",
      "MAXIMUM ACC 0.325\n",
      "active attacker gradien ascent every 8 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.4033333333333333\n",
      "sum of cosine acc 0.40166666666666667\n",
      "sum of grad diff acc 0.39166666666666666\n",
      "sum of grad_diff/loss acc 0.39666666666666667\n",
      "all epoch loss acc 0.37666666666666665\n",
      "all epoch cosine acc 0.35000000000000003\n",
      "all epoch grad diff acc 0.3833333333333333\n",
      "all epoch sum acc 0.38999999999999996\n",
      "all epoch loss+cosine+grad_diff acc 0.3466666666666667\n",
      "sum of loss reduction acc 0.4000000000000001\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.36000000000000004\n",
      "MAXIMUM ACC 0.4033333333333333\n",
      "testing instances included\n",
      "sum of loss acc 0.3075\n",
      "sum of cosine acc 0.29000000000000004\n",
      "sum of grad diff acc 0.27\n",
      "all epoch loss acc 0.28750000000000003\n",
      "all epoch cosine acc 0.26749999999999996\n",
      "all epoch grad diff acc 0.2675\n",
      "all epoch sum acc 0.27\n",
      "all epoch loss+cosine+grad_diff acc 0.27249999999999996\n",
      "sum of loss reduction acc 0.28500000000000003\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.2575\n",
      "MAXIMUM ACC 0.3075\n",
      "active attacker gradien ascent every 9 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.4633333333333333\n",
      "sum of cosine acc 0.4483333333333333\n",
      "sum of grad diff acc 0.42833333333333334\n",
      "sum of grad_diff/loss acc 0.4533333333333333\n",
      "all epoch loss acc 0.4266666666666667\n",
      "all epoch cosine acc 0.41333333333333333\n",
      "all epoch grad diff acc 0.38999999999999996\n",
      "all epoch sum acc 0.47666666666666674\n",
      "all epoch loss+cosine+grad_diff acc 0.41\n",
      "sum of loss reduction acc 0.4666666666666666\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.4266666666666667\n",
      "MAXIMUM ACC 0.47666666666666674\n",
      "testing instances included\n",
      "sum of loss acc 0.34\n",
      "sum of cosine acc 0.3075\n",
      "sum of grad diff acc 0.3125\n",
      "all epoch loss acc 0.32999999999999996\n",
      "all epoch cosine acc 0.295\n",
      "all epoch grad diff acc 0.30000000000000004\n",
      "all epoch sum acc 0.3375\n",
      "all epoch loss+cosine+grad_diff acc 0.33249999999999996\n",
      "sum of loss reduction acc 0.35000000000000003\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.3575\n",
      "MAXIMUM ACC 0.3575\n",
      "active attacker gradien ascent every 10 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.425\n",
      "sum of cosine acc 0.39\n",
      "sum of grad diff acc 0.37333333333333335\n",
      "sum of grad_diff/loss acc 0.38666666666666666\n",
      "all epoch loss acc 0.3833333333333333\n",
      "all epoch cosine acc 0.39666666666666667\n",
      "all epoch grad diff acc 0.3333333333333333\n",
      "all epoch sum acc 0.42333333333333334\n",
      "all epoch loss+cosine+grad_diff acc 0.42\n",
      "sum of loss reduction acc 0.37333333333333335\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.38000000000000006\n",
      "MAXIMUM ACC 0.425\n",
      "testing instances included\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of loss acc 0.3075\n",
      "sum of cosine acc 0.28500000000000003\n",
      "sum of grad diff acc 0.31999999999999995\n",
      "all epoch loss acc 0.2675\n",
      "all epoch cosine acc 0.275\n",
      "all epoch grad diff acc 0.2725\n",
      "all epoch sum acc 0.3025\n",
      "all epoch loss+cosine+grad_diff acc 0.2725\n",
      "sum of loss reduction acc 0.30000000000000004\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.30000000000000004\n",
      "MAXIMUM ACC 0.31999999999999995\n"
     ]
    }
   ],
   "source": [
    "gap = np.array([2,3,4,5,6,7,8,9,10])\n",
    "prefix = '/Users/jclialex/PycharmProjects/whiteboxmi_expdata/expdata/expdata/'\n",
    "epochs = ((np.arange(29)+1)*10)\n",
    "\n",
    "for this_gap in gap:\n",
    "    print (f\"active attacker gradien ascent every {this_gap} epoch\")\n",
    "    all_analysis(epochs,prefix,'cifar10','alexnet',200,num_user=3,gap=this_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active attacker gradien ascent every 2 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.6933333333333334\n",
      "sum of cosine acc 0.6766666666666666\n",
      "sum of grad diff acc 0.7066666666666667\n",
      "sum of grad_diff/loss acc 0.6766666666666666\n",
      "all epoch loss acc 0.64\n",
      "all epoch cosine acc 0.66\n",
      "all epoch grad diff acc 0.64\n",
      "all epoch sum acc 0.7266666666666667\n",
      "all epoch loss+cosine+grad_diff acc 0.7399999999999999\n",
      "sum of loss reduction acc 0.7200000000000001\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.7066666666666667\n",
      "MAXIMUM ACC 0.7399999999999999\n",
      "testing instances included\n",
      "sum of loss acc 0.575\n",
      "sum of cosine acc 0.635\n",
      "sum of grad diff acc 0.5549999999999999\n",
      "all epoch loss acc 0.6849999999999999\n",
      "all epoch cosine acc 0.5800000000000001\n",
      "all epoch grad diff acc 0.565\n",
      "all epoch sum acc 0.605\n",
      "all epoch loss+cosine+grad_diff acc 0.7250000000000001\n",
      "sum of loss reduction acc 0.615\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6799999999999999\n",
      "MAXIMUM ACC 0.7250000000000001\n",
      "active attacker gradien ascent every 3 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.7333333333333333\n",
      "sum of cosine acc 0.7133333333333334\n",
      "sum of grad diff acc 0.6933333333333334\n",
      "sum of grad_diff/loss acc 0.6733333333333333\n",
      "all epoch loss acc 0.6666666666666666\n",
      "all epoch cosine acc 0.7600000000000001\n",
      "all epoch grad diff acc 0.6866666666666665\n",
      "all epoch sum acc 0.7066666666666667\n",
      "all epoch loss+cosine+grad_diff acc 0.6999999999999998\n",
      "sum of loss reduction acc 0.7733333333333333\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.7999999999999999\n",
      "MAXIMUM ACC 0.7999999999999999\n",
      "testing instances included\n",
      "sum of loss acc 0.575\n",
      "sum of cosine acc 0.5700000000000001\n",
      "sum of grad diff acc 0.535\n",
      "all epoch loss acc 0.6100000000000001\n",
      "all epoch cosine acc 0.5599999999999999\n",
      "all epoch grad diff acc 0.62\n",
      "all epoch sum acc 0.565\n",
      "all epoch loss+cosine+grad_diff acc 0.645\n",
      "sum of loss reduction acc 0.605\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6749999999999999\n",
      "MAXIMUM ACC 0.6749999999999999\n",
      "active attacker gradien ascent every 4 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.7166666666666667\n",
      "sum of cosine acc 0.6933333333333334\n",
      "sum of grad diff acc 0.6966666666666667\n",
      "sum of grad_diff/loss acc 0.71\n",
      "all epoch loss acc 0.6266666666666666\n",
      "all epoch cosine acc 0.7200000000000001\n",
      "all epoch grad diff acc 0.6866666666666666\n",
      "all epoch sum acc 0.6666666666666666\n",
      "all epoch loss+cosine+grad_diff acc 0.7466666666666667\n",
      "sum of loss reduction acc 0.7133333333333333\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6533333333333333\n",
      "MAXIMUM ACC 0.7466666666666667\n",
      "testing instances included\n",
      "sum of loss acc 0.56\n",
      "sum of cosine acc 0.545\n",
      "sum of grad diff acc 0.47\n",
      "all epoch loss acc 0.67\n",
      "all epoch cosine acc 0.53\n",
      "all epoch grad diff acc 0.5950000000000001\n",
      "all epoch sum acc 0.59\n",
      "all epoch loss+cosine+grad_diff acc 0.6649999999999999\n",
      "sum of loss reduction acc 0.65\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.665\n",
      "MAXIMUM ACC 0.67\n",
      "active attacker gradien ascent every 5 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.6733333333333333\n",
      "sum of cosine acc 0.65\n",
      "sum of grad diff acc 0.6533333333333333\n",
      "sum of grad_diff/loss acc 0.6433333333333333\n",
      "all epoch loss acc 0.6133333333333334\n",
      "all epoch cosine acc 0.68\n",
      "all epoch grad diff acc 0.6466666666666666\n",
      "all epoch sum acc 0.6533333333333333\n",
      "all epoch loss+cosine+grad_diff acc 0.66\n",
      "sum of loss reduction acc 0.68\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6999999999999998\n",
      "MAXIMUM ACC 0.6999999999999998\n",
      "testing instances included\n",
      "sum of loss acc 0.545\n",
      "sum of cosine acc 0.51\n",
      "sum of grad diff acc 0.49\n",
      "all epoch loss acc 0.6\n",
      "all epoch cosine acc 0.5\n",
      "all epoch grad diff acc 0.55\n",
      "all epoch sum acc 0.605\n",
      "all epoch loss+cosine+grad_diff acc 0.675\n",
      "sum of loss reduction acc 0.5900000000000001\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.5700000000000001\n",
      "MAXIMUM ACC 0.675\n",
      "active attacker gradien ascent every 6 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.6833333333333333\n",
      "sum of cosine acc 0.68\n",
      "sum of grad diff acc 0.68\n",
      "sum of grad_diff/loss acc 0.6733333333333333\n",
      "all epoch loss acc 0.6533333333333333\n",
      "all epoch cosine acc 0.7533333333333333\n",
      "all epoch grad diff acc 0.6866666666666666\n",
      "all epoch sum acc 0.6733333333333333\n",
      "all epoch loss+cosine+grad_diff acc 0.7399999999999999\n",
      "sum of loss reduction acc 0.5933333333333334\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.7133333333333333\n",
      "MAXIMUM ACC 0.7533333333333333\n",
      "testing instances included\n",
      "sum of loss acc 0.5950000000000001\n",
      "sum of cosine acc 0.535\n",
      "sum of grad diff acc 0.475\n",
      "all epoch loss acc 0.625\n",
      "all epoch cosine acc 0.5250000000000001\n",
      "all epoch grad diff acc 0.5950000000000001\n",
      "all epoch sum acc 0.6\n",
      "all epoch loss+cosine+grad_diff acc 0.63\n",
      "sum of loss reduction acc 0.615\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.65\n",
      "MAXIMUM ACC 0.65\n",
      "active attacker gradien ascent every 7 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.69\n",
      "sum of cosine acc 0.74\n",
      "sum of grad diff acc 0.6966666666666667\n",
      "sum of grad_diff/loss acc 0.7066666666666667\n",
      "all epoch loss acc 0.64\n",
      "all epoch cosine acc 0.7133333333333333\n",
      "all epoch grad diff acc 0.7066666666666667\n",
      "all epoch sum acc 0.7533333333333333\n",
      "all epoch loss+cosine+grad_diff acc 0.7533333333333333\n",
      "sum of loss reduction acc 0.6466666666666666\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.7599999999999999\n",
      "MAXIMUM ACC 0.7599999999999999\n",
      "testing instances included\n",
      "sum of loss acc 0.62\n",
      "sum of cosine acc 0.545\n",
      "sum of grad diff acc 0.53\n",
      "all epoch loss acc 0.5700000000000001\n",
      "all epoch cosine acc 0.5449999999999999\n",
      "all epoch grad diff acc 0.63\n",
      "all epoch sum acc 0.575\n",
      "all epoch loss+cosine+grad_diff acc 0.5850000000000001\n",
      "sum of loss reduction acc 0.62\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.645\n",
      "MAXIMUM ACC 0.645\n",
      "active attacker gradien ascent every 8 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.6933333333333334\n",
      "sum of cosine acc 0.66\n",
      "sum of grad diff acc 0.65\n",
      "sum of grad_diff/loss acc 0.7\n",
      "all epoch loss acc 0.6533333333333333\n",
      "all epoch cosine acc 0.64\n",
      "all epoch grad diff acc 0.6666666666666666\n",
      "all epoch sum acc 0.7200000000000001\n",
      "all epoch loss+cosine+grad_diff acc 0.7133333333333333\n",
      "sum of loss reduction acc 0.7000000000000001\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.64\n",
      "MAXIMUM ACC 0.7200000000000001\n",
      "testing instances included\n",
      "sum of loss acc 0.575\n",
      "sum of cosine acc 0.49\n",
      "sum of grad diff acc 0.45999999999999996\n",
      "all epoch loss acc 0.6000000000000001\n",
      "all epoch cosine acc 0.52\n",
      "all epoch grad diff acc 0.605\n",
      "all epoch sum acc 0.5750000000000001\n",
      "all epoch loss+cosine+grad_diff acc 0.64\n",
      "sum of loss reduction acc 0.6200000000000001\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.635\n",
      "MAXIMUM ACC 0.64\n",
      "active attacker gradien ascent every 9 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.7\n",
      "sum of cosine acc 0.7133333333333334\n",
      "sum of grad diff acc 0.6833333333333333\n",
      "sum of grad_diff/loss acc 0.7266666666666667\n",
      "all epoch loss acc 0.6466666666666666\n",
      "all epoch cosine acc 0.7333333333333334\n",
      "all epoch grad diff acc 0.6466666666666667\n",
      "all epoch sum acc 0.7266666666666667\n",
      "all epoch loss+cosine+grad_diff acc 0.6933333333333334\n",
      "sum of loss reduction acc 0.6066666666666667\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6866666666666665\n",
      "MAXIMUM ACC 0.7333333333333334\n",
      "testing instances included\n",
      "sum of loss acc 0.63\n",
      "sum of cosine acc 0.575\n",
      "sum of grad diff acc 0.55\n",
      "all epoch loss acc 0.595\n",
      "all epoch cosine acc 0.545\n",
      "all epoch grad diff acc 0.635\n",
      "all epoch sum acc 0.665\n",
      "all epoch loss+cosine+grad_diff acc 0.625\n",
      "sum of loss reduction acc 0.6000000000000001\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.65\n",
      "MAXIMUM ACC 0.665\n",
      "active attacker gradien ascent every 10 epoch\n",
      "400\n",
      "member only case\n",
      "sum of loss acc 0.71\n",
      "sum of cosine acc 0.68\n",
      "sum of grad diff acc 0.6533333333333333\n",
      "sum of grad_diff/loss acc 0.6366666666666667\n",
      "all epoch loss acc 0.6533333333333333\n",
      "all epoch cosine acc 0.62\n",
      "all epoch grad diff acc 0.5866666666666668\n",
      "all epoch sum acc 0.6866666666666666\n",
      "all epoch loss+cosine+grad_diff acc 0.6866666666666666\n",
      "sum of loss reduction acc 0.68\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6933333333333334\n",
      "MAXIMUM ACC 0.71\n",
      "testing instances included\n",
      "sum of loss acc 0.62\n",
      "sum of cosine acc 0.515\n",
      "sum of grad diff acc 0.535\n",
      "all epoch loss acc 0.635\n",
      "all epoch cosine acc 0.48000000000000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all epoch grad diff acc 0.535\n",
      "all epoch sum acc 0.665\n",
      "all epoch loss+cosine+grad_diff acc 0.61\n",
      "sum of loss reduction acc 0.5800000000000001\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.625\n",
      "MAXIMUM ACC 0.665\n"
     ]
    }
   ],
   "source": [
    "gap = np.array([2,3,4,5,6,7,8,9,10])\n",
    "prefix = '/Users/jclialex/PycharmProjects/whiteboxmi_expdata/expdata/expdata/'\n",
    "epochs = ((np.arange(29)+1)*10)\n",
    "\n",
    "for this_gap in gap:\n",
    "    print (f\"active attacker gradien ascent every {this_gap} epoch\")\n",
    "    all_analysis(epochs,prefix,'cifar100','alexnet',100,num_user=3,gap=this_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active attacker gradien ascent every 2 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.5483333333333333\n",
      "sum of cosine acc 0.565\n",
      "sum of grad diff acc 0.5483333333333333\n",
      "sum of grad_diff/loss acc 0.5533333333333333\n",
      "all epoch loss acc 0.8133333333333334\n",
      "all epoch cosine acc 0.6833333333333332\n",
      "all epoch grad diff acc 0.6966666666666667\n",
      "all epoch sum acc 0.6766666666666667\n",
      "all epoch loss+cosine+grad_diff acc 0.6933333333333334\n",
      "sum of loss reduction acc 0.7966666666666665\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.79\n",
      "MAXIMUM ACC 0.8133333333333334\n",
      "testing instances included\n",
      "sum of loss acc 0.51\n",
      "sum of cosine acc 0.4625\n",
      "sum of grad diff acc 0.395\n",
      "all epoch loss acc 0.6625000000000001\n",
      "all epoch cosine acc 0.5549999999999999\n",
      "all epoch grad diff acc 0.565\n",
      "all epoch sum acc 0.5325\n",
      "all epoch loss+cosine+grad_diff acc 0.6375000000000001\n",
      "sum of loss reduction acc 0.7\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.69\n",
      "MAXIMUM ACC 0.7\n",
      "active attacker gradien ascent every 3 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.705\n",
      "sum of cosine acc 0.6083333333333333\n",
      "sum of grad diff acc 0.6183333333333333\n",
      "sum of grad_diff/loss acc 0.6316666666666667\n",
      "all epoch loss acc 0.7366666666666667\n",
      "all epoch cosine acc 0.6966666666666667\n",
      "all epoch grad diff acc 0.6699999999999999\n",
      "all epoch sum acc 0.71\n",
      "all epoch loss+cosine+grad_diff acc 0.7133333333333334\n",
      "sum of loss reduction acc 0.7533333333333333\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.7533333333333333\n",
      "MAXIMUM ACC 0.7533333333333333\n",
      "testing instances included\n",
      "sum of loss acc 0.5825\n",
      "sum of cosine acc 0.49500000000000005\n",
      "sum of grad diff acc 0.47250000000000003\n",
      "all epoch loss acc 0.6475\n",
      "all epoch cosine acc 0.52\n",
      "all epoch grad diff acc 0.5599999999999999\n",
      "all epoch sum acc 0.5774999999999999\n",
      "all epoch loss+cosine+grad_diff acc 0.66\n",
      "sum of loss reduction acc 0.6699999999999999\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.67\n",
      "MAXIMUM ACC 0.67\n",
      "active attacker gradien ascent every 4 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.6083333333333333\n",
      "sum of cosine acc 0.5283333333333333\n",
      "sum of grad diff acc 0.5416666666666666\n",
      "sum of grad_diff/loss acc 0.54\n",
      "all epoch loss acc 0.5733333333333334\n",
      "all epoch cosine acc 0.53\n",
      "all epoch grad diff acc 0.5366666666666666\n",
      "all epoch sum acc 0.58\n",
      "all epoch loss+cosine+grad_diff acc 0.5166666666666667\n",
      "sum of loss reduction acc 0.6166666666666667\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.5966666666666667\n",
      "MAXIMUM ACC 0.6166666666666667\n",
      "testing instances included\n",
      "sum of loss acc 0.45749999999999996\n",
      "sum of cosine acc 0.415\n",
      "sum of grad diff acc 0.43000000000000005\n",
      "all epoch loss acc 0.535\n",
      "all epoch cosine acc 0.4225\n",
      "all epoch grad diff acc 0.42249999999999993\n",
      "all epoch sum acc 0.48250000000000004\n",
      "all epoch loss+cosine+grad_diff acc 0.4575\n",
      "sum of loss reduction acc 0.47000000000000003\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.4675\n",
      "MAXIMUM ACC 0.535\n",
      "active attacker gradien ascent every 5 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.6966666666666667\n",
      "sum of cosine acc 0.6016666666666667\n",
      "sum of grad diff acc 0.6183333333333333\n",
      "sum of grad_diff/loss acc 0.63\n",
      "all epoch loss acc 0.6566666666666667\n",
      "all epoch cosine acc 0.6533333333333333\n",
      "all epoch grad diff acc 0.6666666666666666\n",
      "all epoch sum acc 0.6866666666666666\n",
      "all epoch loss+cosine+grad_diff acc 0.6833333333333332\n",
      "sum of loss reduction acc 0.6900000000000001\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6900000000000001\n",
      "MAXIMUM ACC 0.6966666666666667\n",
      "testing instances included\n",
      "sum of loss acc 0.5475\n",
      "sum of cosine acc 0.47000000000000003\n",
      "sum of grad diff acc 0.4775\n",
      "all epoch loss acc 0.5900000000000001\n",
      "all epoch cosine acc 0.48999999999999994\n",
      "all epoch grad diff acc 0.525\n",
      "all epoch sum acc 0.5650000000000001\n",
      "all epoch loss+cosine+grad_diff acc 0.62\n",
      "sum of loss reduction acc 0.595\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.64\n",
      "MAXIMUM ACC 0.64\n",
      "active attacker gradien ascent every 6 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.6516666666666666\n",
      "sum of cosine acc 0.585\n",
      "sum of grad diff acc 0.63\n",
      "sum of grad_diff/loss acc 0.6383333333333333\n",
      "all epoch loss acc 0.6466666666666666\n",
      "all epoch cosine acc 0.6333333333333333\n",
      "all epoch grad diff acc 0.6366666666666667\n",
      "all epoch sum acc 0.6433333333333333\n",
      "all epoch loss+cosine+grad_diff acc 0.6733333333333333\n",
      "sum of loss reduction acc 0.6666666666666666\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6333333333333333\n",
      "MAXIMUM ACC 0.6733333333333333\n",
      "testing instances included\n",
      "sum of loss acc 0.5525\n",
      "sum of cosine acc 0.4525\n",
      "sum of grad diff acc 0.44499999999999995\n",
      "all epoch loss acc 0.5725\n",
      "all epoch cosine acc 0.4625\n",
      "all epoch grad diff acc 0.5675\n",
      "all epoch sum acc 0.5625\n",
      "all epoch loss+cosine+grad_diff acc 0.55\n",
      "sum of loss reduction acc 0.56\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6025\n",
      "MAXIMUM ACC 0.6025\n",
      "active attacker gradien ascent every 7 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.685\n",
      "sum of cosine acc 0.6666666666666666\n",
      "sum of grad diff acc 0.6683333333333333\n",
      "sum of grad_diff/loss acc 0.6783333333333333\n",
      "all epoch loss acc 0.66\n",
      "all epoch cosine acc 0.7033333333333333\n",
      "all epoch grad diff acc 0.6766666666666667\n",
      "all epoch sum acc 0.6633333333333333\n",
      "all epoch loss+cosine+grad_diff acc 0.7033333333333333\n",
      "sum of loss reduction acc 0.6833333333333332\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.7033333333333333\n",
      "MAXIMUM ACC 0.7033333333333333\n",
      "testing instances included\n",
      "sum of loss acc 0.625\n",
      "sum of cosine acc 0.49750000000000005\n",
      "sum of grad diff acc 0.4525\n",
      "all epoch loss acc 0.625\n",
      "all epoch cosine acc 0.515\n",
      "all epoch grad diff acc 0.5575\n",
      "all epoch sum acc 0.5775\n",
      "all epoch loss+cosine+grad_diff acc 0.63\n",
      "sum of loss reduction acc 0.61\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6449999999999999\n",
      "MAXIMUM ACC 0.6449999999999999\n",
      "active attacker gradien ascent every 8 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.6366666666666667\n",
      "sum of cosine acc 0.5916666666666667\n",
      "sum of grad diff acc 0.6083333333333333\n",
      "sum of grad_diff/loss acc 0.6266666666666667\n",
      "all epoch loss acc 0.5866666666666666\n",
      "all epoch cosine acc 0.6033333333333334\n",
      "all epoch grad diff acc 0.5766666666666667\n",
      "all epoch sum acc 0.66\n",
      "all epoch loss+cosine+grad_diff acc 0.6266666666666666\n",
      "sum of loss reduction acc 0.6699999999999999\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.63\n",
      "MAXIMUM ACC 0.6699999999999999\n",
      "testing instances included\n",
      "sum of loss acc 0.5525\n",
      "sum of cosine acc 0.44\n",
      "sum of grad diff acc 0.4425\n",
      "all epoch loss acc 0.5650000000000001\n",
      "all epoch cosine acc 0.4425\n",
      "all epoch grad diff acc 0.49749999999999994\n",
      "all epoch sum acc 0.5475\n",
      "all epoch loss+cosine+grad_diff acc 0.555\n",
      "sum of loss reduction acc 0.5725\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.5975\n",
      "MAXIMUM ACC 0.5975\n",
      "active attacker gradien ascent every 9 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.6883333333333334\n",
      "sum of cosine acc 0.655\n",
      "sum of grad diff acc 0.6833333333333333\n",
      "sum of grad_diff/loss acc 0.7\n",
      "all epoch loss acc 0.6733333333333333\n",
      "all epoch cosine acc 0.6666666666666666\n",
      "all epoch grad diff acc 0.6966666666666667\n",
      "all epoch sum acc 0.7033333333333333\n",
      "all epoch loss+cosine+grad_diff acc 0.6833333333333332\n",
      "sum of loss reduction acc 0.68\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6999999999999998\n",
      "MAXIMUM ACC 0.7033333333333333\n",
      "testing instances included\n",
      "sum of loss acc 0.625\n",
      "sum of cosine acc 0.47750000000000004\n",
      "sum of grad diff acc 0.505\n",
      "all epoch loss acc 0.6224999999999999\n",
      "all epoch cosine acc 0.5175000000000001\n",
      "all epoch grad diff acc 0.6224999999999999\n",
      "all epoch sum acc 0.6375\n",
      "all epoch loss+cosine+grad_diff acc 0.6475\n",
      "sum of loss reduction acc 0.6525000000000001\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6775\n",
      "MAXIMUM ACC 0.6775\n",
      "active attacker gradien ascent every 10 epoch\n",
      "800\n",
      "member only case\n",
      "sum of loss acc 0.625\n",
      "sum of cosine acc 0.5883333333333334\n",
      "sum of grad diff acc 0.6183333333333333\n",
      "sum of grad_diff/loss acc 0.64\n",
      "all epoch loss acc 0.6166666666666667\n",
      "all epoch cosine acc 0.56\n",
      "all epoch grad diff acc 0.5966666666666667\n",
      "all epoch sum acc 0.6666666666666666\n",
      "all epoch loss+cosine+grad_diff acc 0.63\n",
      "sum of loss reduction acc 0.6433333333333334\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.6166666666666667\n",
      "MAXIMUM ACC 0.6666666666666666\n",
      "testing instances included\n",
      "sum of loss acc 0.5425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of cosine acc 0.45\n",
      "sum of grad diff acc 0.43\n",
      "all epoch loss acc 0.5725\n",
      "all epoch cosine acc 0.4525\n",
      "all epoch grad diff acc 0.52\n",
      "all epoch sum acc 0.56\n",
      "all epoch loss+cosine+grad_diff acc 0.5575\n",
      "sum of loss reduction acc 0.5800000000000001\n",
      "all epoch loss+cosine+grad_diff+loss_reduction acc 0.5975\n",
      "MAXIMUM ACC 0.5975\n"
     ]
    }
   ],
   "source": [
    "gap = np.array([2,3,4,5,6,7,8,9,10])\n",
    "prefix = '/Users/jclialex/PycharmProjects/whiteboxmi_expdata/expdata/expdata/'\n",
    "epochs = ((np.arange(29)+1)*10)\n",
    "\n",
    "for this_gap in gap:\n",
    "    print (f\"active attacker gradien ascent every {this_gap} epoch\")\n",
    "    all_analysis(epochs,prefix,'cifar100','alexnet',200,num_user=3,gap=this_gap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
